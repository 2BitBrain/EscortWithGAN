{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger(' -Owakati /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "##this function reads corpus from read_path.\n",
    "def read_corpus(read_path):\n",
    "    with open(read_path) as fs:\n",
    "        lines = fs.readlines()\n",
    "    return [line.split(\"\\n\")[0] for line in lines]\n",
    "    \n",
    "# !!!wakati is used for only japanese!!! this function split pharse level and save sentence splited.\n",
    "def wakati(read_path, save_path):\n",
    "    with open(read_path) as fs:\n",
    "        lines = fs.readlines()\n",
    "    lines = [mecab.parse(line).split(\"\\n\")[0] for line in lines]\n",
    "    print(lines[:10])\n",
    "    with open(save_path, \"w\") as fs:\n",
    "        fs.write(\"\\n\".join(lines))\n",
    "        \n",
    "##this function return vocabulary set(vocabulary dictionary) from wakatied corpus.\n",
    "def mk_dict_from_wakatied(read_path):\n",
    "    with open(read_path) as fs:\n",
    "        lines = fs.readlines()\n",
    "    word = []\n",
    "    lines = [ mecab.parse(line).split(\"\\n\")[0] for line in lines]\n",
    "    [[word.append(word_) for word_ in line.split(\" \")] for line in lines]\n",
    "    return list(set(word))\n",
    "\n",
    "##this function save vocabulary set(vocabulary dictionary).\n",
    "def save_index(file_name, words):\n",
    "    words = '\\n'.join(words)\n",
    "    with open(file_name, \"a\") as fs:\n",
    "        fs.write(words)\n",
    "\n",
    "##this function read vocabulary set(vocabulary dictionary) from saved.\n",
    "def read_index(read_path):\n",
    "    with open(read_path, \"r\") as fs:\n",
    "        lines = fs.readlines()\n",
    "    lines = [line.split(\"\\n\")[0] for line in lines]\n",
    "    return lines\n",
    "    \n",
    "## this function marge 2 vocabulary sets.\n",
    "def marge_vocab(A_vocabs, B_vocabs):\n",
    "    words = []\n",
    "    for a_v, b_v in zip(A_vocabs, B_vocabs):\n",
    "        words.append(a_v)\n",
    "        worda.append(b_v)\n",
    "    return list(set(words))\n",
    "\n",
    "## Initial input for decoder\n",
    "def mk_go(batch_size, vocab_size, embedding):\n",
    "    r = []\n",
    "    for _ in range(batch_size):\n",
    "        if embedding:\n",
    "            r.append(vocab_size)\n",
    "        else:\n",
    "            c = [0]*(vocab_size+2)\n",
    "            c[vocab_size] = 1\n",
    "            r.append(c)\n",
    "    return np.reshape(r, (-1, 1)) if embedding else np.array(r)\n",
    "\n",
    "## this function convert sentence to index which is index of marged vocabrary.\n",
    "def convert_sentence2index(sentences, index, time_step, go = False):\n",
    "    r = []\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        words = sentence.split(\" \")\n",
    "        converted = [index.index(word) for word in words]\n",
    "        if go:\n",
    "            converted.insert(0, len(index))\n",
    "        while len(converted) != time_step and len(converted) <= time_step:\n",
    "            converted.append(len(index)+1)\n",
    "        r.append(converted[:time_step])\n",
    "    return np.reshape(np.array(r), (-1, time_step, 1))\n",
    "\n",
    "##this function conver sentence to one_hot_encoded  vector ..\n",
    "def convert_sentence2one_hot_encoding(sentences, indexs, time_step, go=False):\n",
    "    r = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split(\" \")\n",
    "        time_steps = []\n",
    "        ## append <GO>\n",
    "        if go:\n",
    "            content = [0]*(len(indexs)+2)\n",
    "            content[len(indexs)] = 1\n",
    "            time_steps.append(content)\n",
    "        \n",
    "        for word in words:\n",
    "            content = [0]*(len(indexs)+2)\n",
    "            idx = indexs.index(word)\n",
    "            content[idx] = 1\n",
    "            time_steps.append(content)\n",
    "\n",
    "        ##append <EOS>\n",
    "        while len(time_steps) <= time_step and len(time_steps) != time_step:\n",
    "            content = [0]*(len(indexs)+2)\n",
    "            content[len(indexs)+1] = 1\n",
    "            time_steps.append(content)\n",
    "\n",
    "        r.append(time_steps[:time_step])\n",
    "    return np.array(r)\n",
    "\n",
    "## this function return fuction that yields training data for each time steps. And this function used for pre_training.\n",
    "def mk_training_func(A_wakatied_path, B_wakatied_path, A_vocabs_path, B_vocabs_path, batch_size, time_step, embedding=True):\n",
    "    ## loading reshaped data.. reshaped meas that sentence is splited white space.\n",
    "    A_corpus = read_corpus(A_wakatied_path)\n",
    "    B_corpus = read_corpus(B_wakatied_path)\n",
    "    \n",
    "    ## Loading each vocabulary and marge them\n",
    "    indexs = marge_vocab(read_index(A_vocabs_path), read_index(B_vocabs_path))\n",
    "    \n",
    "    if embedding:\n",
    "        convert_func = convert_sentence2index\n",
    "    else:\n",
    "        convert_func = convert_sentence2one_hot_encoding\n",
    "    \n",
    "    def pre_training_func():\n",
    "        while True:\n",
    "            A_choiced_idx = [random.choice(A_corpus) for _ in range(batch_size)]\n",
    "            B_choiced_idx = [random.choice(B_corpus) for _ in range(batch_size)]\n",
    "        \n",
    "            A_in = convert_f11unc(A_choiced_idx, indexs, time_step)\n",
    "            A_d_in = convert_func(A_choiced_idx, indexs, time_step, True)\n",
    "            A_d_label = convert_sentence2one_hot_encoding(A_choiced_idx, indexs, time_step)[:,:,:]\n",
    "            \n",
    "            B_in = convert_func(B_choiced_idx, indexs, time_step)\n",
    "            B_d = convert_func(B_choiced_idx, indexs, time_step, True)\n",
    "            B_d_label = convert_sentence2one_hot_encoding(B_choiced_idx, indexs, time_step)[:,:,:] \n",
    "            yield A_in, A_d_in, A_d_label, B_in, B_d, B_d_label\n",
    "        \n",
    "    def training_func():\n",
    "        while True:\n",
    "            A_choiced_idx = [random.choice(A_corpus) for _ in range(batch_size)]\n",
    "            B_choiced_idx = [random.choice(B_corpus) for _ in range(batch_size)]\n",
    "            \n",
    "            A_in = convert_func(A_choiced_idx, indexs, time_step)\n",
    "            B_in = convert_func(B_choiced_idx, indexs, time_step)\n",
    "        yield A_in, B_in\n",
    "    return pre_training_func, training_func"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
